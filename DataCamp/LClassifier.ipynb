{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d45299-dafb-41e7-a9e3-1a02cc88fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "newsgroups = sklearn.datasets.fetch_20newsgroups_vectorized()\n",
    "X, y = newsgroups.data, newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444d9e8-58ad-4bc0-8a99-4ca4c4fd24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test)[0]\n",
    "print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30503286-a7ab-4962-a493-705505ec0926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/godwin/Desktop/Organised/python/python/machine_learning_dir/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9555555555555556\n",
      "0.9955456570155902\n",
      "0.9822222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab8316-366c-46fd-8434-7e9a85ea1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X,y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cae13-b7cd-431e-8577-29f8315a0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squared error, summed over training examples\n",
    "from scipy.optimize import minimize\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true - y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425eb35-3a9c-4bea-bc58-49538129dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbd5f3-c725-4771-899b-15b2ce3a612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weak_reg = LogisticRegression(C=100)\n",
    "lr_strong_reg = LogisticRegression(C=0.01)\n",
    "lr_weak_reg.fit(X_train, y_train)\n",
    "lr_strong_reg.fit(X_train, y_train)\n",
    "lr_weak_reg.score(X_train, y_train)\n",
    "lr_strong_reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2c736-2a42-4374-a045-fe16725b9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_L1 = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "lr_L2 = LogisticRegression() # penalty='l2' by default\n",
    "lr_L1.fit(X_train, y_train)\n",
    "lr_L2.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(lr_L1.coef_.flatten())\n",
    "plt.plot(lr_L2.coef_.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ac7b5-7d20-4918-88b5-636fd590ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C = C_value)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2dba26-207a-4f80-8caf-295e8377c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(solver='liblinear',penalty='l1')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594685cb-f966-4641-9a6d-6ca77503f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e123bb8-bb72-4580-b7d3-6716b0a74ae3",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b92d75-212b-4422-837e-8a888c1d6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-vs-rest\n",
    "lr_ovr = LogisticRegression(multi_class='ovr')\n",
    "\n",
    "lr_ovr.fit(X,y)\n",
    "lr_ovr.coef_.shape\n",
    "\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\")\n",
    "lr_mn.fit(X,y)\n",
    "lr_mn.coef_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe4e43-94fe-4294-9258-ae8082f54e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression(multi_class='ovr')\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eceeeb0-79d4-4d19-9dfe-b4a52b103669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training accuracies\n",
    "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "lr_class_1 = LogisticRegression(C=100)\n",
    "lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "plot_classifier(X_train, y_train==1, lr_class_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5dc14-59ce-48a7-b67d-85c16219a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X, y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small, y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81489021-7e4a-4f89-89a2-4b6cbec0527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma=1)\n",
    "# default is kernel=\"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fbcf1-c713-46b5-81d8-1984f18d9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X,y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad029b2c-bef2-4201-9d75-14794bce5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e489db-4e03-422c-b449-16d41c5ac235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss': [\"hinge\", \"log\"], 'penalty': [\"l1\", \"l2\"]}\n",
    "searcher = GridSearchCV(linear_classifier, param_grid=parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e876a-945d-4bbb-9fd3-e0912202b0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
